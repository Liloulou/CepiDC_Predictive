import tensorflow as tf
import subprocess
import pickle
import numpy as np
import my_input_fn as pipe
import model 
from tensorflow.contrib import feature_column
import os

params = {
    'batch_size': 200,
    'num_epochs': 500,
    'train': {
        'learning_rate': 0.001,
        'decay_rate': 0.98,
        'decay_steps': 2000
    },
    'inference': {
        'misc': {
            'units': [50, 30],
            'keep_prob': [0.01, 0.01]
        },
        'date': {
            'filters': [50, 50],
            'kernel': [2, 3],
            'units': [50, 30],
            'keep_prob': [0.01, 0.01]
        },
        'loc': {
            'filters': [70, 70],
            'kernel': [2, 3],
            'units': [50, 30],
            'keep_prob': [0.01, 0.01]
        },
        'cause': [
            {
                'filters': 700,
                'kernel': 2,
                'keep_prob': 0.01
            },
            {
                'filters': 700,
                'kernel': 3,
                'keep_prob': 0.01
            },
            {
                'filters': 700,
                'kernel': 4,
                'keep_prob': 0.01
            }
        ],
        'decoder': {
            'dilation': [
                {
                    'filters': [500, 500],
                    'kernel': [3, 4],
                    'keep_prob': 0.01
                },
                {
                    'filters': [500, 500],
                    'kernel': [3, 4],
                    'keep_prob': 0.01
                },
                {
                    'filters': [500, 500],
                    'kernel': [3, 4],
                    'keep_prob': 0.01
                },
                {
                    'filters': [500, 500],
                    'kernel': [3, 4],
                    'keep_prob': 0.01
                }
            ],
            'decoding': {
                'bah_units': 100,
                'lstm_units': 100,
                'attention_units': 100
            }
        }
    }
}

hparams = {
    'batch_size': 100,
    'num_epochs': 500,
    'train': {
        'learning_rate': 0.001,
        'decay_rate': 0.98,
        'decay_steps': 2000
    },
    'inference': {
        'misc': {
            'units': [50, 30],
            'keep_prob': [0.01, 0.01]
        },
        'date': {
            'filters': [50, 50],
            'kernel': [2, 3],
            'units': [50, 30],
            'keep_prob': [0.01, 0.01]
        },
        'loc': {
            'filters': [70, 70],
            'kernel': [2, 3],
            'units': [50, 30],
            'keep_prob': [0.01, 0.01]
        },
        'cause': {
            'block': [
                {
                    'dimension': 2,
                    'filters': [1000, 1000],
                    'kernel': [3, 3],
                    'drop_out': 0.01
                },
                {
                    'dimension': 2,
                    'filters': [1000, 1000],
                    'kernel': [3, 3],
                    'drop_out': 0.01
                },
                {
                    'dimension': 2,
                    'filters': [1000, 1000],
                    'kernel': [3, 3],
                    'drop_out': 0.01
                },
                {
                    'dimension': 2,
                    'filters': [1000, 1000],
                    'kernel': [3, 3],
                    'drop_out': 0.01
                }
            ],
            'units': 1000,
            'drop_out': 0.01
        },
        'encoder': {
            'units': [1000, 1000, 1000]
        },
        'decoder': {
            'bahdanau': 500,
            'units': [1000, 1000],
            }
        }
    }

model_dir = 'model_directory/'

loc_keys = ['loc_' + x for x in ['dom', 'dc', 'nais']]
date_keys = ['date_' + x for x in['dc', 'nais']]
miscellaneous = ['sexe', 'activ', 'etatmat', 'lieudc', 'jvecus']
misc_bucket_sizes = {'sexe': 2, 'activ': 3, 'etatmat': 4, 'lieudc': 6}
cause_chain = ['c' + str(x) for x in range(1, 25)]
vocab_list = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',
              'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J',
              'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',
              'U', 'V', 'W', 'X', 'Y', 'Z']
mean_dict = {'date_nais': [[1934.6344833246696], [6.395205434766596], [15.677822136824627]],
             'date_dc': [[2013], [6.298270277778363], [15.675359823892013]],
             'jvecus': 28620.254809402246}
std_dev_dict = {'date_nais': [[16.200837191507265], [3.4454328128621263], [8.818802916420342]],
                'date_dc': [[0.0000001], [3.5486695665231016], [8.791450686446204]],
                'jvecus': 5914.239761843669}
cause_name_list = ['c' + str(i) for i in range(0, 25)]


def get_next_model_dir():

    list_name = [int(name[name.find('_') + 1:]) for name in os.listdir('model_directory')]

    if len(list_name) is 0:
        last_model = 0
    else:
        last_model = max(list_name)

    return 'model_directory/model_' + str(last_model + 1)


def make_hparams():

    # search granularity
    step = 5

    # drop out parameters
    drop_out_1 = int(np.random.uniform(1, 8)) * 0.1
    drop_out_2 = int(np.random.uniform(1, 8)) * 0.1
    drop_out_3 = int(np.random.uniform(1, 8)) * 0.1

    # cause network pre-definition
    cause_max_val = 500
    lay_per_block_cause = np.random.binomial(1, 0.5) + 2

    # recurrent encoder/decoder pre-definition
    rec_max_val = 500
    decoder_len = int(np.random.uniform(1, 4))
    encoder_len = np.minimum(4, decoder_len + int(np.random.uniform(0, 4)))

    hparams = {}
    hparams['batch_size'] = int(np.random.uniform(32, 100))
    hparams['num_epochs'] = 500
    hparams['train'] = {
        'learning_rate': 10 ** -int(np.random.uniform(2, 4)),
        'decay_rate': np.random.uniform(0.95, 1),
        'decay_steps': int(np.random.uniform(1500, 4000))
    }
    bla = {}
    bla['cause'] = {
        'block': [
            {
                'dimension': 2,
                'filters': [int(np.random.uniform(1, step + 1) * cause_max_val // step)] * lay_per_block_cause,
                'kernel': [int(np.random.uniform(2, 4))] * lay_per_block_cause,
                'drop_out': drop_out_1
            },
            {
                'dimension': 2,
                'filters': [int(np.random.uniform(1, step + 1) * cause_max_val // step)] * lay_per_block_cause,
                'kernel': [int(np.random.uniform(2, 4))] * lay_per_block_cause,
                'drop_out': drop_out_1
            },
            {
                'dimension': 2,
                'filters': [int(np.random.uniform(1, step + 1) * cause_max_val // step)] * lay_per_block_cause,
                'kernel': [int(np.random.uniform(2, 4))] * lay_per_block_cause,
                'drop_out': drop_out_1
            },
            {
                'dimension': 2,
                'filters': [int(np.random.uniform(1, step + 1) * cause_max_val // step)] * lay_per_block_cause,
                'kernel': [int(np.random.uniform(2, 4))] * lay_per_block_cause,
                'drop_out': drop_out_1
            },
        ],
        'units': int(np.random.uniform(1, step)) * cause_max_val // step,
        'drop_out': drop_out_1
    }
    bla['encoder'] = {
        'units': [int(np.random.uniform(1, step + 1) * rec_max_val // step)] * encoder_len,
        'drop_out': drop_out_2,
    }
    bla['decoder'] = {
        'bahdanau': int(np.random.uniform(1, step + 1) * rec_max_val // step),
        'units': [int(np.random.uniform(1, step + 1) * rec_max_val // step)] * decoder_len,
        'drop_out': drop_out_3,
    }
    hparams['inference'] = bla

    return hparams


def input_normalizer(tensor, key):
    """
    Normalizes a qualitative variable 0 mean and unit variance
    :param tensor: an ND tensor
    :param key: the key identifying the tensor variable in the dataset dict
    :return: the mean-var normalized tensor
    """
    return (tensor - mean_dict[key]) / std_dev_dict[key]


def make_columns():

    # build feature_column for date variables
    with tf.name_scope('date_columns'):

        date_columns = []

        date_columns.append(
            tf.feature_column.numeric_column(
                date_keys[0],
                shape=(3, 1),
                normalizer_fn=lambda x: input_normalizer(x, date_keys[0])
            ))

        date_columns.append(
            tf.feature_column.numeric_column(
                date_keys[1],
                shape=(3, 1),
                normalizer_fn=lambda x: input_normalizer(x, date_keys[1])
            ))

    # build feature_columns for localisation variables
    with tf.name_scope('loc_columns'):

        loc_columns = []

        for key in loc_keys:
            loc_columns.append(tf.feature_column.indicator_column(
                feature_column.sequence_categorical_column_with_vocabulary_list(key, vocab_list, default_value=0)))

    # build feature_columns for miscellaneous variables
    with tf.name_scope('misc_columns'):

        misc_columns = []

        for misc_key in miscellaneous:
            if misc_key != 'jvecus':
                misc_columns.append(
                    tf.feature_column.indicator_column(
                        tf.feature_column.categorical_column_with_identity(
                            misc_key,
                            num_buckets=misc_bucket_sizes[misc_key]
                        )))

            else:
                misc_columns.append(
                    tf.feature_column.numeric_column(
                        misc_key,
                        normalizer_fn=lambda x: input_normalizer(x, misc_key)))

    # build feature columns for causal chain variables
    with tf.name_scope('cause_columns'):

        cause_columns = []

        for key in cause_chain:
            cause_columns.append(tf.feature_column.indicator_column(
                feature_column.sequence_categorical_column_with_vocabulary_list(key, vocab_list, default_value=0)))

    # build feature column for the labels
    with tf.name_scope('labels_one_hot_encoding'):

        label_column = tf.feature_column.indicator_column(
            feature_column.sequence_categorical_column_with_vocabulary_list('labels', vocab_list, default_value=0))

    columns_dict = {
        'date': date_columns,
        'loc': loc_columns,
        'misc': misc_columns,
        'cause': cause_columns,
        'labels': label_column
    }

    return columns_dict


tf.logging.set_verbosity(tf.logging.INFO)

for i in range(20):

    hparams = make_hparams()
    next_model_dir = get_next_model_dir()

    hparams['feature_columns'] = make_columns()
    run_config = tf.estimator.RunConfig(
        model_dir=get_next_model_dir(),
        save_checkpoints_steps=500,
        keep_checkpoint_max=1,
    )

    estimator = tf.estimator.Estimator(
        model_fn=model.get_model_fn,
        config=run_config,
        params=hparams
    )

    train_spec = tf.estimator.TrainSpec(
        input_fn=lambda: pipe.really_simple_input_fn('train', hparams['batch_size'], hparams['num_epochs']),
        max_steps=100000,
        hooks=[tf.contrib.estimator.stop_if_no_decrease_hook(
            estimator,
            metric_name='loss',
            max_steps_without_decrease=10000,
            run_every_secs=None,
            run_every_steps=500
        )]
    )

    eval_spec = tf.estimator.EvalSpec(
        input_fn=lambda: pipe.really_simple_input_fn('valid', hparams['batch_size'], 20),
        steps=100,
        start_delay_secs=60,
        throttle_secs=120
    )

    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
    pickle.dump(
        hparams,
        open(
            'model_directory/' + next_model_dir + '/h_parameters_' + next_model_dir[next_model_dir.find('_') + 1:],
            'wb'
        )
    )
